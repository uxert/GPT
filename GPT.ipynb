{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Notebook inspired by Andrej Karpathy's film: https://www.youtube.com/watch?v=kCc8FmEb1nY&",
   "id": "2ca48070df5db43e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:44:00.836412Z",
     "start_time": "2024-11-30T15:43:58.716684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gdown  # aby pobrać zbiór danych z dysku google\n",
    "import os.path\n",
    "\n",
    "from torch import tensor\n",
    "import torch"
   ],
   "id": "fced0524376200ab",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Zbiór danych jest dowolny tak długo, jak długo jest tekstowy. \n",
    "Na początek naszy zbiór będzie bardzo prosty - jeden plik tekstowy zawierający dzieła Szekspira."
   ],
   "id": "f5ed096824492474"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Zbiór danych jest pobierany z dysku google (jeśli nie jest już pobrany) a następnie wczytywany jako zwykły plik tekstowy",
   "id": "730c919dc03341b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-30T15:44:00.871833Z",
     "start_time": "2024-11-30T15:44:00.862045Z"
    }
   },
   "source": [
    "dataset_link = \"https://drive.google.com/uc?id=1TQjhbN1jrQx7eMgySFkMfwahh7IZy2a8\"\n",
    "dataset_path = \"data/Shakespeare.txt\"\n",
    "if not os.path.isfile(dataset_path):\n",
    "    print(\"Downloading dataset...\")\n",
    "    gdown.download(dataset_link, dataset_path)\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(\"Dataset is already downloaded\")\n",
    "    \n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    whole_text = f.read()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is already downloaded\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:44:00.993508Z",
     "start_time": "2024-11-30T15:44:00.989054Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Length of the dataset: {len(whole_text)}\")",
   "id": "ad7b1cec47a6ea4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset: 1115394\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\"Podejrzymy\" początek naszego zbioru danych",
   "id": "e3cf09dc6b56309c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:44:01.045542Z",
     "start_time": "2024-11-30T15:44:01.039793Z"
    }
   },
   "cell_type": "code",
   "source": "print(whole_text[:1000])",
   "id": "1c2807f37ec79569",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Nasz LLM, jak każda inna sieć neuronowa, operuje na liczbach. Potrzebny nam jest enkoder, który pozwoli zamienić tekst na liczby oraz dekoder, który odwróci działanie enkodera.\n",
    "\n",
    "Aby zdefiniować jakikolwiek enkoder potrzebny nam jest kompletny zbiór słownictwa. Aby zachować prostotę, ograniczymy się tutaj do pojedyńczych liter - jedna litera będzie jednym tokenem. Niemniej należy pamiętać, że w bardziej zaawansowanych modelach dużo lepiej działają tokeny składające się ze zlepków liter.\n",
    "\n",
    "Podział tekstu na tokeny jest wielkim problemem samym w sobie i nie będziemy się w niego zagłębiać w ramach tego projektu"
   ],
   "id": "8a67f3b066256f4d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:44:01.165927Z",
     "start_time": "2024-11-30T15:44:01.131144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab = sorted(list(set(whole_text)))  # set zapewnia unikalność znaków, lista daje się posortować\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab: {\"\".join(vocab)}\")\n",
    "print(f\"Vocab len: {len(vocab)}\")"
   ],
   "id": "5a8a05d01d858226",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab len: 65\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:44:01.223964Z",
     "start_time": "2024-11-30T15:44:01.218195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stoi = {char: i for i, char in enumerate(vocab)}\n",
    "itos = {i: char for i, char in enumerate(vocab)}\n",
    "encode = lambda s: [stoi[c] for c in s]  # zamienia string na listę liczb\n",
    "decode = lambda l: \"\".join(itos[i] for i in l)\n",
    "\n",
    "print(encode(\"Hello world!\"))\n",
    "print(decode(encode(\"Hello world!\")))"
   ],
   "id": "84990ece0e822902",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2]\n",
      "Hello world!\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Wczytajmy cały zbiór danych (w końcu waży tylko 1MB) do tensora",
   "id": "f7555b3807b46116"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:44:01.489011Z",
     "start_time": "2024-11-30T15:44:01.275779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "data = tensor(encode(whole_text))\n",
    "print(f\"{data.shape = }, {data.dtype = }\")\n",
    "print(data[:100])"
   ],
   "id": "a462693afc854373",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape = torch.Size([1115394]), data.dtype = torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Podzielimy nasz zbiór danych na część testową i walidacyjną aby móc ocenić, na ile dobrze model uczy się ogólnych tendencji, a nie po prostu \"zapamiętuje\".  Zazwyczaj zbiory danych dzielone są losowo, tak, aby żadna z \"części\" takiego zbioru nie była słabiej reprezentowana. \n",
    "\n",
    "W tym przypadku wyciąganie wyrazów ze środka tekstu nie będzie dobrą metodą, ponieważ w analizie tekstu kluczowy jest kontekst. W związku z tym tekst zostanie \"przedzielony\" na dwie części"
   ],
   "id": "6c1fad7a07d469f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:44:01.518275Z",
     "start_time": "2024-11-30T15:44:01.513854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_to_all_ratio = 0.85\n",
    "n = int(train_to_all_ratio * data.numel())\n",
    "train_data, test_data = data[:n], data[n:]\n",
    "print(f\"{len(train_data) = }, {len(test_data) = }\")"
   ],
   "id": "6f320acc2ec931e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_data) = 948084, len(test_data) = 167310\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Przy ćwiczeniu modeli językowych nie można podać całego zbioru danych na raz. Tekst jest podawany w losowo wybieranych blokach.\n",
    "Wielkość takiego bloku jest ustalona i jest jednocześnie **maksymalnym** rozmiarem kontekstu dostępnego dla naszego modelu. Dla każdego bloku model jest ćwiczony dla wszystkich kontekstów zaczynając od 1 a kończąc na `block_size`.\n",
    "\n",
    "Zobrazujmy to za pomocą pętli:"
   ],
   "id": "98e8ec8d5cfe5748"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:44:01.649774Z",
     "start_time": "2024-11-30T15:44:01.638785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_size = 8\n",
    "x = train_data[:context_size]\n",
    "y = train_data[1:context_size + 1]  # cel jest przesunięty o 1 względem danej treningowej\n",
    "for i in range(context_size):\n",
    "    context = x[:i+1]  # do i-tego znaku włącznie\n",
    "    target = y[i]\n",
    "    print(f\"For input {context} the target is {target}\")"
   ],
   "id": "b428450b959e3e49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For input tensor([18]) the target is 47\n",
      "For input tensor([18, 47]) the target is 56\n",
      "For input tensor([18, 47, 56]) the target is 57\n",
      "For input tensor([18, 47, 56, 57]) the target is 58\n",
      "For input tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "For input tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "For input tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "For input tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ćwiczenie modelu na wszystkich rozmiarach kontekstu, oprócz oczywistej zalety w postaci szybkości uzyskiwania danych (sekwencja tekstu i tak jest pozyskana), pozwala nauczyć model działania także przy krótszych kontekstach. W naszym przypadku kontekst nie będzie duży, jednak w przypadku takich modeli jak GPT-3.5 czy GPT-4 rozmiar kontekstu może wynosić nawet setki tysięcy tokenów i większość zapytań nie będzie tak długa",
   "id": "f9d04a3834305b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Karty graficzne są bardzo dobre w zrównoleglaniu obliczeń - dlatego podczas korzystania z modeli podaje im się wiele niezależnych wartości, które są przetwarzane jednocześnie. Zbiór wszystkich takich wartości nazywany jest batchem",
   "id": "fae5eb50ed98cb76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:44:01.804810Z",
     "start_time": "2024-11-30T15:44:01.790171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 64\n",
    "\n",
    "rng = torch.Generator()\n",
    "rng.manual_seed(42)\n",
    "\n",
    "def get_random_batch(split: str, batch_size:int, block_size: int, rng: torch.Generator) -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    Returns a random batch of data - tensor of shape (batch_size, block_size). \n",
    "    :param split: can be either \"train\" or \"test\". When train the train dataset is used\n",
    "    :returns: A tuple of two tensors - first with training data and second with labels/targets\n",
    "    \"\"\"\n",
    "    if split == \"train\": my_data = train_data\n",
    "    elif split == \"test\": my_data = test_data\n",
    "    else: raise ValueError(f\"Expected either `train` or `test` for the split argument, got {split} instead\")\n",
    "    idx = torch.randint(len(my_data) - block_size, size=(batch_size,), generator=rng)\n",
    "    # randint jest end-exclusive, dlatego nie trzeba modyfikować indeksów mimo że target jest i+1\n",
    "    x = torch.stack([data[i: i+block_size] for i in idx])\n",
    "    y = torch.stack([data[i+1: i+block_size+1] for i in idx])\n",
    "    return x, y\n",
    "    \n",
    "xb, yb = get_random_batch(\"train\", batch_size, context_size, rng)\n",
    "print(f\"Data shape: {xb.shape}\")\n",
    "print(f\"Label shape: {yb.shape}\")"
   ],
   "id": "6ec46ed5db6e8ec4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([64, 8])\n",
      "Label shape: torch.Size([64, 8])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Zwracam uwagę, że tutaj dane treningowe oraz targety mają ten sam kształt - dla każdego jednego znaku chcemy wiedzieć, jaki znak jest następny.\n",
    "\n",
    "Można zauważyć, że jest to niepotrzebne duplikowanie informacji, ponieważ dla wszystkich znaków oprócz ostatniego jest ona zawarta już w danych treningowych. To prawda, jednak przy ilości obliczeń wykonywanych przez transformery nie robi to zauważalnej różnicy w wydajności treningu, a kod jest bardziej czytelny"
   ],
   "id": "6285a273e4d05265"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:44:01.871330Z",
     "start_time": "2024-11-30T15:44:01.868501Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2063475a0fa5958b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
